from typing import TypedDict, List, Annotated, Literal,  Union, Dict
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage, SystemMessage
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from langgraph.graph import StateGraph, START, END

from reflector_agent import ReflectorAgent
from aggregator_agent import AggregatorAgent
from debator_agent import DebatorAgent

class AgentState(TypedDict):
    messages: List[BaseMessage]
    query: str
    context: str
    loop_history: List[Dict[str, str]]
    next_agent: Literal["aggregator", "debator", "reflector", "retriever", "end"]
    sub_queries: str # Query generated by orchesteration agent for the next agent

class OrchestrationAgent():
    def __init__(self, model: str, collection_name: str = "documents"):
        self.llm = ChatOllama(
            model=model
        )

        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # Initialize local Qdrant client
        self.qdrant_client = QdrantClient(url="http://localhost:6333")
        
        # Initialize Qdrant vectorstore
        self.vectorstore = QdrantVectorStore(
            client=self.qdrant_client,
            collection_name=collection_name,
            embedding=self.embeddings
        )

        self.reflector_agent = ReflectorAgent(model=model)
        self.aggregator_agent = AggregatorAgent(model=model)
        self.debator_agent = DebatorAgent(model=model)
        
        self.graph = self._build_graph()

    def _build_graph(self):

        workflow = StateGraph(AgentState)

        workflow.add_node("aggregate", self._aggregate)
        workflow.add_node("debate", self._debate)
        workflow.add_node("reflect", self._reflect)
        workflow.add_node("retrieve", self._retrieve)

        workflow.add_node("orchestrator", self._orchestrate)

        workflow.add_edge(START, "orchestrator")
        workflow.add_conditional_edges(
            "orchestrator", lambda state: state["next_agent"], 
            {
                "end": END,
                "aggregator": "aggregate",
                "debator": "debate",
                "reflector": "reflect",
                "retriever": "retrieve",   
            }
        )

        workflow.add_edge("aggregate", "orchestrator")
        workflow.add_edge("debate", "orchestrator")
        workflow.add_edge("reflect", "orchestrator")
        workflow.add_edge("retrieve", "orchestrator")

        return workflow.compile()

    def _orchestrate(self, state: AgentState) -> AgentState:
        """
        Orchestrator that decides which agent to call next
        """
        messages = state.get("messages", [])
        loop_history = state.get("loop_history", [])
        query = state.get("query", "")
        context = state.get("context", "")
        
        # Count how many times each agent has been used
        agent_counts = {}
        for entry in loop_history:
            agent = entry.get("agent", "")
            agent_counts[agent] = agent_counts.get(agent, 0) + 1
        
        
        system_prompt = f"""You are an orchestrator deciding which agent to use next in a multi-agent system.

        Current query: {query}
        Current context: {context if context else "None"}
        Agents usage history: {[entry["agent"] for entry in loop_history]}
        Agent usage counts: {agent_counts}
        Last agent: {loop_history[-1]["agent"] if loop_history else "None"}

        AGENT PURPOSES:
        - retriever: Gets relevant information from vector database when context is missing or insufficient
        - debator: Generates multiple perspectives and conducts structured debate for complex/controversial topics
        - aggregator: Generates multiple responses and synthesizes consistent information into a single answer
        - reflector: Reviews and improves existing responses to reduce hallucination and increase quality

        DECISION RULES:
        1. Use 'retriever' if there is insufficient context and the query needs factual information
        2. Use 'debator' for complex, controversial, or opinion-based topics that benefit from multiple perspectives
        3. Use 'aggregator' when you have context and need a consistent, fact-based response
        4. Use 'reflector' to improve quality after generating an initial response (use sparingly, max 1-2 times)
        5. Avoid infinite loops - limit each agent to 2-3 uses maximum
        6. Choose 'end' when you have a comprehensive, high-quality final response

        Respond with only: retriever, debator, aggregator, reflector, or end"""

        orchestrator_messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"What agent should be used next?")
        ]
        
        response = self.llm.invoke(orchestrator_messages)
        agent_choice = response.content.strip().lower()
        
        print(f"Orchestrator chose: {agent_choice} (Loop: {len(loop_history)}")
        
        # Prevent infinite loops - max 8 total loops
        if len(loop_history) >= 8:
            print("Max loops reached, ending workflow")
            state["next_agent"] = None
            return state
        
        # Map response to valid agent names
        if "retriever" in agent_choice and agent_counts.get("retriever", 0) < 3:
            state["next_agent"] = "retriever"
        elif "debator" in agent_choice and agent_counts.get("debator", 0) < 3:
            state["next_agent"] = "debator"
        elif "aggregator" in agent_choice and agent_counts.get("aggregator", 0) < 3:
            state["next_agent"] = "aggregator"
        elif "reflector" in agent_choice and agent_counts.get("reflector", 0) < 3:
            state["next_agent"] = "reflector"
        else:
            state["next_agent"] = "end"
        
        return state

    def _aggregate(self, state: AgentState) -> AgentState:
        """
        Aggregator agent that generates multiple responses and synthesizes consistent information
        """
        print("Aggregator called")
        
        # Update loop history
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "aggregator"})
        state["loop_history"] = loop_history
        
        query = state.get("query", "")
        context = state.get("context", "")
        messages = state.get("messages", [])
        
        # Use aggregator agent to generate consistent response
        aggregated_response = self.aggregator_agent.generate_response(
            query=query,
            context=context
        )
        
        print(f"Aggregator generated response: {aggregated_response[:100]}...")
        
        # Add aggregated response to messages
        messages.append(AIMessage(content=aggregated_response))
        state["messages"] = messages
        
        return state
    
    def _debate(self, state: AgentState) -> AgentState:
        """
        Debator agent that generates multiple perspectives and conducts structured debate
        """
        print("Debator called")
        
        # Update loop history
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "debator"})
        state["loop_history"] = loop_history
        
        query = state.get("query", "")
        context = state.get("context", "")
        messages = state.get("messages", [])
        
        # Use debator agent to generate multi-perspective response
        debate_response = self.debator_agent.generate_response(
            query=query,
            context=context
        )
        
        print(f"Debator generated response: {debate_response[:100]}...")
        
        # Add debate response to messages
        messages.append(AIMessage(content=debate_response))
        state["messages"] = messages
        
        return state
    
    def _reflect(self, state: AgentState) -> AgentState:
        """
        Reflection agent that criticizes the current response and improves it
        """
        print("Reflector called")

        # Update loop history
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "reflector"})
        state["loop_history"] = loop_history
        
        
        query = state.get("query", "")
        context = state.get("context", "")
        messages = state.get("messages", [])
        
        # Get the current response (last AI message)
        current_response = ""
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                current_response = msg.content
                break
        
        if not current_response:
            print("No previous response to reflect on")
            return state
        
        # Use reflector agent to improve the response
        improved_response = self.reflector_agent.generate_response(
            query=query,
            current_response=current_response,
            context=context
        )
        
        print(f"Reflector generated improved response: {improved_response[:100]}...")
        
        # Add improved response to messages
        messages.append(AIMessage(content=improved_response))
        state["messages"] = messages
        
        return state
    
    def _retrieve(self, state: AgentState) -> AgentState:
        """
        Retrieval agent that searches vector database using the query
        """
        print("Retriever called")
        
        query = state.get("query", "")
        loop_history = state.get("loop_history", [])
        
        # Add to loop history
        loop_history.append({
            "agent": "retriever",
            "query": query
        })
        state["loop_history"] = loop_history
        
        # Perform vector search
        docs = self.vectorstore.similarity_search(query, k=5)
        
        # Build context
        if docs:
            context_parts = [f"Document {i+1}: {doc.page_content}" for i, doc in enumerate(docs)]
            context = f"Retrieved information for query: '{query}'\n\n" + "\n\n".join(context_parts)
            print(f"- Found {len(docs)} relevant documents")
        else:
            context = f"Query: '{query}'\n\nNo relevant documents found in the knowledge base."
            print(f"- Found no relevant documents")
        
        state["context"] = context
                
        return state

    def generate_response(self, query: str):
        # Initialize state properly
        initial_state = {
            "query": query,
            "context": "",
            "messages": [],
            "loop_history": [],
            "next_agent": None,
            "sub_queries": ""
        }
        
        print(f"Starting orchestration for query: {query}")
        response = self.graph.invoke(initial_state)

        print(f"Final orchestration completed. Total agents used: {len(response.get('loop_history', []))}")
        
        # Get the final response from messages
        messages = response.get("messages", [])
        if messages:
            for msg in reversed(messages):
                if isinstance(msg, AIMessage):
                    return msg.content
        
        return "No response generated"

    def save_workflow_image(self):
        """Save workflow as PNG image"""
        try:
            png_data = self.graph.get_graph().draw_mermaid_png()
            
            with open("orchestration_workflow.png", "wb") as f:
                f.write(png_data)
                        
        except Exception as e:
            print(f"Install graphviz: pip install graphviz")


if __name__ == "__main__":
    # Local Qdrant instance (make sure to run: docker run -p 6333:6333 qdrant/qdrant)
    orchestration_agent = OrchestrationAgent(
        model="mistral:7b", 
        collection_name="my_documents"
    )

    # Test with a complex/controversial question
    response1 = orchestration_agent.generate_response("Should artificial intelligence be regulated by government?")
    print(f"\nComplex Topic Response:\n{response1}")
    
    # Test with a factual question  
    response2 = orchestration_agent.generate_response("Who is the author of Harry Potter?")
    print(f"\nFactual Question Response:\n{response2}")