from typing import TypedDict, List, Annotated, Literal,  Union, Dict
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage, SystemMessage
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langgraph.graph import StateGraph, START, END

from reflector_agent import ReflectorAgent
from aggregator_agent import AggregatorAgent

class AgentState(TypedDict):
    messages: List[BaseMessage]
    query: str
    context: str
    loop_history: List[Dict[str, str]]
    next_agent: Union[Literal["aggregator", "debator", "reflector", "retriever"], None]
    sub_queries: str # Query generated by orchesteration agent for the next agent

class OrchestrationAgent():
    def __init__(self, model: str):
        self.llm = ChatOllama(
            model=model
        )

        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        self.vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings
        )

        self.reflector_agent = ReflectorAgent(model=model)
        self.aggregator_agent = AggregatorAgent(model=model)
        
        self.graph = self._build_graph()

    def _build_graph(self):

        workflow = StateGraph(AgentState)

        workflow.add_node("aggregate", self._aggregate)
        workflow.add_node("debate", self._debate)
        workflow.add_node("reflect", self._reflect)
        workflow.add_node("retrieve", self._retrieve)

        workflow.add_node("orchestrator", self._orchestrate)

        workflow.add_edge(START, "orchestrator")
        workflow.add_conditional_edges(
            "orchestrator", self._choose_agent, 
            {
                "end": END,
                "aggregator": "aggregate",
                "debator": "debate",
                "reflector": "reflect",
                "retriever": "retrieve",   
            }
        )

        workflow.add_edge("aggregate", "orchestrator")
        workflow.add_edge("debate", "orchestrator")
        workflow.add_edge("reflect", "orchestrator")
        workflow.add_edge("retrieve", "orchestrator")

        return workflow.compile()

    def _orchestrate(self, state: AgentState) -> AgentState:
        """
        Orchestrator that decides which agent to call next based on conversation state
        """
        messages = state.get("messages", [])
        loop_history = state.get("loop_history", [])
        query = state.get("query", "")
        context = state.get("context", "")
        
        system_prompt = f"""You are an orchestrator deciding which agent to use next.
        
        Current context: {context if context else "None"}
        Last agent used: {loop_history[-1]["agent"] if loop_history[-1]["agent"] else "No agent used yet"}
        Loop history: {[entry['agent'] for entry in loop_history]}

        Choose the next agent based on these rules:
        - retriever: When you need more information or context
        - debator: When you need to explore different perspectives or arguments
        - reflector: When you need to decrease hallucination or increase quality of the response 
        - aggregator: When you have enough information and need to compile final response
        - None: When the task is complete

        Respond with only the agent name or 'None'."""

        orchestrator_messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"What agent should be used next for this query: {query}")
        ]
        
        response = self.llm.invoke(orchestrator_messages)
        agent_choice = response.content.strip().lower()
        
        # Map response to valid agent names
        if "retriever" in agent_choice:
            state["next_agent"] = "retriever"
        elif "debator" in agent_choice:
            state["next_agent"] = "debator"
        elif "reflector" in agent_choice:
            state["next_agent"] = "reflector"
        elif "aggregator" in agent_choice:
            state["next_agent"] = "aggregator"
        else:
            state["next_agent"] = None
        
        return state

    def _aggregate(self, state: AgentState) -> AgentState: # under progress, don't edit
        print("Aggregator called")
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "aggregator"})
        state["loop_history"] = loop_history
        return state
    
    def _debate(self, state: AgentState) -> AgentState: # under progress, don't edit
        print("Debator called")
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "debator"})
        state["loop_history"] = loop_history
        return state
    
    def _reflect(self, state: AgentState) -> AgentState:
        """
        Reflection agent that criticizes the current response and improves it
        """
        print("Reflector called")

        # Update loop history
        loop_history = state.get("loop_history", [])
        loop_history.append({"agent": "reflector"})
        state["loop_history"] = loop_history
        
        
        query = state.get("query", "")
        context = state.get("context", "")
        messages = state.get("messages", [])
        
        # Get the current response (last AI message)
        current_response = ""
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                current_response = msg.content
                break
        
        # Use reflector agent to improve the response
        improved_response = self.reflector_agent.generate_response(
            query=query,
            current_response=current_response,
            context=context
        )
        
        # Add improved response to messages
        messages.append(AIMessage(content=improved_response))
        state["messages"] = messages
        
        return state
    
    def _retrieve(self, state: AgentState) -> AgentState:
        """
        Retrieval agent that searches vector database using orchestrator's query
        """
        print("Retriever called")
        loop_history = state.get("loop_history", [])
        sub_query = state.get("sub_queries")[-1]
        
        # Add to loop history
        loop_history.append({
            "agent": "retriever",
            "sub_query": sub_query
        })
        state["loop_history"] = loop_history
        
        # Perform vector search
        docs = self.vectorstore.similarity_search(sub_query, k=3)
        
        # Build context
        if docs:
            context_parts = [f"Document {i+1}: {doc.page_content}" for i, doc in enumerate(docs)]
            context = f"Query: '{sub_query}'\n\n" + "\n\n".join(context_parts)
            print(f"- Found {len(docs)} documents")
        else:
            context = f"Query: '{sub_query}'\n\nNo relevant documents found."
            print(f"- Found no documents")
        
        state["context"] = context
                
        return state
    
    def _choose_agent(self, state: AgentState) -> str:
        if state["next_agent"]:
            return state["next_agent"]
        else:
            return "end"

    def generate_response(self, query: str):
        response = self.graph.invoke({
            "query": query,
        })

        print("Agent Response: ", response)

        return response.content
    

    def save_workflow_image(self):
        """Save workflow as PNG image"""
        try:
            png_data = self.graph.get_graph().draw_mermaid_png()
            
            with open("orchestration_workflow.png", "wb") as f:
                f.write(png_data)
                        
        except Exception as e:
            print(f"Install graphviz: pip install graphviz")


if __name__ == "__main__":
    orchestration_agent = OrchestrationAgent(model="mistral:7b")

    response = orchestration_agent.generate_response("Who is the author of Harry Potter?")

    print(response)