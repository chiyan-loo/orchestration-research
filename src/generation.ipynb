{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mounty-ed/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hotpot_qa\", \n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=\"/mnt/d/datasets/hotpot_qa\"\n",
    ")\n",
    "\n",
    "hf_df = pd.DataFrame(dataset)\n",
    "hf_df = hf_df.sample(n=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760067705.783884   26615 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760067705.790827   26615 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760067705.792177   26615 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760067705.793620   26615 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from agents.prompt_and_workflow_orchestration.orchestration import OrchestrationAgent\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"hotpot_qa_cot_{MODEL_NAME}\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')\n",
    "\n",
    "\n",
    "# planner_llm = ChatOllama(model=\"qwen3:8b\", temperature=0.6)\n",
    "\n",
    "planner_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.7)\n",
    "high_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.8)\n",
    "medium_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.5)\n",
    "low_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.2)\n",
    "\n",
    "agent = OrchestrationAgent(\n",
    "    planner_llm=planner_llm,\n",
    "    high_temp_llm=high_temp_llm,\n",
    "    medium_temp_llm=medium_temp_llm,\n",
    "    low_temp_llm=low_temp_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cost based on Gemini model and token usage.\n",
    "    Uses public pricing as of mid-2025.\n",
    "    \"\"\"\n",
    "    # Pricing data keyed by full model name or prefix\n",
    "    pricing = {\n",
    "        \"gemini-2.0-flash\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        # Use “pro” special logic below for gemini-2.5-pro\n",
    "    }\n",
    "\n",
    "    # Normalize model name to lower\n",
    "    m = model_name.lower()\n",
    "\n",
    "    # Special case: gemini-2.5-pro tiered pricing\n",
    "    if m.startswith(\"gemini-2.5-pro\"):\n",
    "        # threshold check on input token count\n",
    "        if input_tokens > 200_000:\n",
    "            in_rate = 2.50\n",
    "            out_rate = 15.00\n",
    "        else:\n",
    "            in_rate = 1.25\n",
    "            out_rate = 10.00\n",
    "        return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n",
    "\n",
    "    # Other known models\n",
    "    if m in pricing:\n",
    "        in_rate = pricing[m][\"input\"]\n",
    "        out_rate = pricing[m][\"output\"]\n",
    "    else:\n",
    "        # fallback / unknown handling\n",
    "        print(f\"Warning: Unknown model '{model_name}'. Using zero cost.\")\n",
    "        in_rate = 0.0\n",
    "        out_rate = 0.0\n",
    "\n",
    "    return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/300 [00:00<?, ?it/s]E0000 00:00:1760067705.839411   26615 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Processing:   3%|▎         | 10/300 [03:42<2:07:10, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for item 9: 'NoneType' object has no attribute 'reasoning'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 50/300 [18:38<1:38:11, 23.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error for item 49: 'NoneType' object has no attribute 'reasoning'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [1:51:54<00:00, 22.38s/it]  \n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "generated_dataset = []\n",
    "hf_df = hf_df.reset_index(drop=True)\n",
    "\n",
    "for i, item in tqdm(hf_df.iterrows(), total=len(hf_df), desc=\"Processing\"):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    context = item[\"context\"][\"sentences\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "    try:\n",
    "        # Silence the agent's output\n",
    "        with redirect_stdout(StringIO()):\n",
    "            response = await agent.generate_response_async(query=question, context=context, callback=callback)\n",
    "\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "\n",
    "        input_tokens = callback.usage_metadata[MODEL_NAME][\"input_tokens\"]\n",
    "        output_tokens = callback.usage_metadata[MODEL_NAME][\"output_tokens\"]\n",
    "        total_tokens = callback.usage_metadata[MODEL_NAME][\"total_tokens\"]\n",
    "        cost = calculate_cost(MODEL_NAME, input_tokens, output_tokens)\n",
    "\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"user_input\": question,\n",
    "                \"contexts\": [str(item) for item in context],\n",
    "                \"response\": response[\"content\"].strip(),\n",
    "                \"ground_truth\": answer,\n",
    "                \"workflow_plan\": response[\"workflow_plan\"],\n",
    "                \"planner_reasoning\": response[\"planner_reasoning\"],\n",
    "                \"custom_prompts\": response[\"custom_prompts\"],\n",
    "                \"latency\": latency,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"cost\": cost,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors and continue\n",
    "        print(f\"Unexpected error for item {i}: {e}\")\n",
    "\n",
    "        # Add entry with error information\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"user_input\": question,\n",
    "                \"contexts\": [str(item) for item in context],\n",
    "                \"response\": f\"ERROR: {str(e)}\",\n",
    "                \"ground_truth\": answer,\n",
    "                \"workflow_plan\": None,\n",
    "                \"planner_reasoning\": None,\n",
    "                \"custom_prompts\": None,\n",
    "                \"latency\": 0,\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "                \"cost\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(generated_dataset)\n",
    "df.to_parquet(generated_data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
