{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 rows\n",
      "Columns: ['user_input', 'contexts', 'response', 'ground_truth', 'workflow_plan']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "EXPERIMENT_NAME = \"hotpot_qa_orchestration_2\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_parquet(generated_data_path)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize LLM Judge\n",
    "judge_llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "print(\"LLM Judge initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Prompt and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation prompt\n",
    "EVAL_PROMPT = \"\"\"You are an impartial judge evaluating the correctness of an AI-generated answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth Answer: {ground_truth}\n",
    "\n",
    "Generated Answer: {response}\n",
    "\n",
    "Determine if the generated answer is correct by comparing it to the ground truth.\n",
    "The answer should be considered correct if it conveys the same key information as the ground truth, even if worded differently.\n",
    "\n",
    "Provide your evaluation in this exact format:\n",
    "Verdict: [True/False]\n",
    "Reasoning: [brief explanation of why the answer is correct or incorrect]\"\"\"\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_answer(row):\n",
    "    prompt = EVAL_PROMPT.format(\n",
    "        question=row['user_input'],\n",
    "        ground_truth=row['ground_truth'],\n",
    "        response=row['response']\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = judge_llm.invoke(prompt)\n",
    "        content = result.content\n",
    "        \n",
    "        # Parse verdict\n",
    "        verdict_line = [line for line in content.split('\\n') if 'Verdict:' in line][0]\n",
    "        verdict_str = verdict_line.split(':')[1].strip().lower()\n",
    "        is_correct = 'true' in verdict_str\n",
    "        \n",
    "        # Parse reasoning\n",
    "        reasoning_line = [line for line in content.split('\\n') if 'Reasoning:' in line][0]\n",
    "        reasoning = reasoning_line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return is_correct, reasoning\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating row: {e}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        return None, str(e)\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [05:31<00:00, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Starting evaluation...\\n\")\n",
    "correctness_results = []\n",
    "reasonings = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating\"):\n",
    "    is_correct, reasoning = evaluate_answer(row)\n",
    "    correctness_results.append(1 if is_correct == True else 0)\n",
    "    reasonings.append(reasoning)\n",
    "\n",
    "# Add results to dataframe\n",
    "df['is_correct'] = correctness_results\n",
    "df['reasoning'] = reasonings\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: /home/mounty-ed/stuff/orchestration_research/data/evaluated/hotpot_qa_orchestration_2_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "output_path = os.path.join(project_root, 'data', 'evaluated', f'{EXPERIMENT_NAME}_eval.parquet')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Total Questions: 24\n",
      "Successfully Evaluated: 24\n",
      "Correct Answers: 19\n",
      "Incorrect Answers: 5\n",
      "Accuracy: 79.17%\n",
      "============================================================\n",
      "\n",
      "Correctness Distribution:\n",
      "is_correct\n",
      "1    19\n",
      "0     5\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>workflow_plan</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Were Scott Derrickson and Ed Wood of the same ...</td>\n",
       "      <td>[['Ed Wood is a 1994 American biographical per...</td>\n",
       "      <td>Yes, Scott Derrickson (born in Los Angeles, Ca...</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'beginning': [], 'end': ['aggregator', 'refin...</td>\n",
       "      <td>1</td>\n",
       "      <td>The generated answer correctly confirms that b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What government position was held by the woman...</td>\n",
       "      <td>[[\"Meet Corliss Archer, a program from radio's...</td>\n",
       "      <td>Shirley Temple Black portrayed Corliss Archer ...</td>\n",
       "      <td>Chief of Protocol</td>\n",
       "      <td>{'beginning': [], 'end': ['aggregator', 'refin...</td>\n",
       "      <td>0</td>\n",
       "      <td>The generated answer incorrectly states that S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What science fantasy young adult series, told ...</td>\n",
       "      <td>[['The Andre Norton Award for Young Adult Scie...</td>\n",
       "      <td>For science fantasy young adult series narrate...</td>\n",
       "      <td>Animorphs</td>\n",
       "      <td>{'beginning': [], 'end': ['aggregator', 'refin...</td>\n",
       "      <td>1</td>\n",
       "      <td>The generated answer correctly identifies \"Ani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are the Laleli Mosque and Esma Sultan Mansion ...</td>\n",
       "      <td>[['Esma Sultan (21 March 1873 – 7 May 1899) wa...</td>\n",
       "      <td>The Laleli Mosque and Esma Sultan Mansion are ...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'beginning': [], 'end': ['aggregator', 'refin...</td>\n",
       "      <td>1</td>\n",
       "      <td>The generated answer correctly states that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The director of the romantic comedy \"Big Stone...</td>\n",
       "      <td>[['Just Another Romantic Wrestling Comedy is a...</td>\n",
       "      <td>Adriana Trigiani, the director of 'Big Stone G...</td>\n",
       "      <td>Greenwich Village, New York City</td>\n",
       "      <td>{'beginning': ['summarizer'], 'end': ['aggrega...</td>\n",
       "      <td>1</td>\n",
       "      <td>The generated answer correctly identifies Gree...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Were Scott Derrickson and Ed Wood of the same ...   \n",
       "1  What government position was held by the woman...   \n",
       "2  What science fantasy young adult series, told ...   \n",
       "3  Are the Laleli Mosque and Esma Sultan Mansion ...   \n",
       "4  The director of the romantic comedy \"Big Stone...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [['Ed Wood is a 1994 American biographical per...   \n",
       "1  [[\"Meet Corliss Archer, a program from radio's...   \n",
       "2  [['The Andre Norton Award for Young Adult Scie...   \n",
       "3  [['Esma Sultan (21 March 1873 – 7 May 1899) wa...   \n",
       "4  [['Just Another Romantic Wrestling Comedy is a...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Yes, Scott Derrickson (born in Los Angeles, Ca...   \n",
       "1  Shirley Temple Black portrayed Corliss Archer ...   \n",
       "2  For science fantasy young adult series narrate...   \n",
       "3  The Laleli Mosque and Esma Sultan Mansion are ...   \n",
       "4  Adriana Trigiani, the director of 'Big Stone G...   \n",
       "\n",
       "                       ground_truth  \\\n",
       "0                               yes   \n",
       "1                 Chief of Protocol   \n",
       "2                         Animorphs   \n",
       "3                                no   \n",
       "4  Greenwich Village, New York City   \n",
       "\n",
       "                                       workflow_plan  is_correct  \\\n",
       "0  {'beginning': [], 'end': ['aggregator', 'refin...           1   \n",
       "1  {'beginning': [], 'end': ['aggregator', 'refin...           0   \n",
       "2  {'beginning': [], 'end': ['aggregator', 'refin...           1   \n",
       "3  {'beginning': [], 'end': ['aggregator', 'refin...           1   \n",
       "4  {'beginning': ['summarizer'], 'end': ['aggrega...           1   \n",
       "\n",
       "                                           reasoning  \n",
       "0  The generated answer correctly confirms that b...  \n",
       "1  The generated answer incorrectly states that S...  \n",
       "2  The generated answer correctly identifies \"Ani...  \n",
       "3  The generated answer correctly states that the...  \n",
       "4  The generated answer correctly identifies Gree...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy (excluding None values)\n",
    "valid_results = [r for r in correctness_results if r is not None]\n",
    "correct_count = sum(valid_results)\n",
    "accuracy = (correct_count / len(valid_results) * 100) if valid_results else 0\n",
    "\n",
    "print(f\"=\"*60)\n",
    "print(f\"EVALUATION SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total Questions: {len(df)}\")\n",
    "print(f\"Successfully Evaluated: {len(valid_results)}\")\n",
    "print(f\"Correct Answers: {correct_count}\")\n",
    "print(f\"Incorrect Answers: {len(valid_results) - correct_count}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nCorrectness Distribution:\")\n",
    "print(df['is_correct'].value_counts())\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
