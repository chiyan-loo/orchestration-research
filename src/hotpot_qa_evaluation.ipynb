{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "EXPERIMENT_NAME = \"hotpot_qa_orchestration\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    AnswerCorrectness\n",
    ")\n",
    "\n",
    "# init metrics with evaluator LLM\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "evaluator_llm = ChatOllama(\n",
    "    model=\"mistral:7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "\n",
    "evaluator_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(evaluator_embeddings)\n",
    "\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerCorrectness(llm=evaluator_llm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "# Load the CSV you created\n",
    "df = pd.read_parquet(generated_data_path)\n",
    "\n",
    "# Store the workflow_plan column before dropping it\n",
    "workflow_plans = df[['workflow_plan']].copy()\n",
    "\n",
    "# Convert to RAGAS dataset format\n",
    "ragas_dataset_from_csv = EvaluationDataset.from_pandas(df.drop(\"workflow_plan\", axis=1))\n",
    "\n",
    "# Evaluate using the CSV data\n",
    "result = evaluate(\n",
    "    metrics=metrics,\n",
    "    dataset=ragas_dataset_from_csv,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings\n",
    ")\n",
    "\n",
    "# Save evaluation results\n",
    "df_results = result.to_pandas()\n",
    "\n",
    "# Add the workflow_plan column back by concatenating\n",
    "# This assumes the order is preserved (which it should be)\n",
    "df_results['workflow_plan'] = workflow_plans['workflow_plan'].values\n",
    "\n",
    "# Save with the workflow_plan column included\n",
    "df_results.to_csv(os.path.join(project_root, 'data', 'evaluated', f'{EXPERIMENT_NAME}_eval.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
