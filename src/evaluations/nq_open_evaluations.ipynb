{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = load_dataset(\n",
    "    \"google-research-datasets/nq_open\", \n",
    "    split=\"validation\",\n",
    "    cache_dir=\"/mnt/d/datasets/nq_open\"\n",
    ")\n",
    "\n",
    "hf_dataset = hf_dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the `QueryEngine`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Documents and their embeddings are not loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m question = item[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m answer = item[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m relevant_docs = \u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_most_relevant_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m response = rag.generate_answer(question, relevant_docs)\n\u001b[32m     13\u001b[39m ragas_dataset.append(\n\u001b[32m     14\u001b[39m     {\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m\"\u001b[39m:question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     }\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/RAG_research/simple_rag.py:29\u001b[39m, in \u001b[36mRAG.get_most_relevant_docs\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Find the most relevant document for a given query.\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.docs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.doc_embeddings:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDocuments and their embeddings are not loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m query_embedding = \u001b[38;5;28mself\u001b[39m.embeddings.embed_query(query)\n\u001b[32m     32\u001b[39m similarities = [\n\u001b[32m     33\u001b[39m     np.dot(query_embedding, doc_emb)\n\u001b[32m     34\u001b[39m     / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc_emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.doc_embeddings\n\u001b[32m     36\u001b[39m ]\n",
      "\u001b[31mValueError\u001b[39m: Documents and their embeddings are not loaded."
     ]
    }
   ],
   "source": [
    "from agents.rag import RAG\n",
    "import pandas as pd\n",
    "\n",
    "rag = RAG()\n",
    "ragas_dataset = []\n",
    "\n",
    "for i, item in enumerate(hf_dataset):\n",
    "    print(f\"{i+1} iteration\")\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "\n",
    "    response = rag.generate_response(question)\n",
    "    ragas_dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": response[\"relevant_docs\"],\n",
    "            \"response\": response[\"content\"],\n",
    "            \"reference\": answer\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(ragas_dataset)\n",
    "df.to_csv(\"../data/generated/rag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    AnswerCorrectness\n",
    ")\n",
    "\n",
    "# init metrics with evaluator LLM\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm),\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm),\n",
    "    AnswerCorrectness(llm=evaluator_llm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64557c0ec02f4c239e14582c0e4eb055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Query Engine:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa06449c07ab43df92932c3cf512467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "evaluator_llm = ChatOllama(\n",
    "    model=\"mistral:7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    metrics=metrics,\n",
    "    dataset=ragas_dataset,\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()\n",
    "df.to_csv(\"../data/evaluated/rag_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
