{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))  # Add project root to path\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', 'hotpot_qa_workflow_plan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = load_dataset(\n",
    "    \"hotpot_qa\", \n",
    "    \"distractor\",\n",
    "    split=\"validation[:100]\",\n",
    "    cache_dir=\"/mnt/d/datasets/hotpot_qa\"\n",
    ")\n",
    "\n",
    "hf_dataset = hf_dataset.select(range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the `QueryEngine`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration\n",
      "2 iteration\n",
      "3 iteration\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "from agents.experimental.orchestration_agent import OrchestrationAgent\n",
    "import pandas as pd\n",
    "\n",
    "agent = OrchestrationAgent(model=\"mistral:7b\")\n",
    "ragas_dataset = []\n",
    "\n",
    "for i, item in enumerate(hf_dataset):\n",
    "    print(f\"{i+1} iteration\")\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    context = item[\"context\"][\"sentences\"]\n",
    "\n",
    "    # Silence the agent's output\n",
    "    with redirect_stdout(StringIO()):\n",
    "        response = agent.generate_response(question, context)\n",
    "        \n",
    "    ragas_dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": [str(item) for item in context],\n",
    "            \"response\": response[\"content\"].strip().replace('\\n', ' '),\n",
    "            \"reference\": answer\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(ragas_dataset)\n",
    "df.to_csv(generated_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    AnswerCorrectness\n",
    ")\n",
    "\n",
    "# init metrics with evaluator LLM\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "evaluator_llm = ChatOllama(\n",
    "    model=\"mistral:7b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerCorrectness(llm=evaluator_llm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SingleTurnSample\nretrieved_contexts\n  Input should be a valid list [type=list_type, input_value='[\\'[\\\\\\'Ed Wood is a 199...g car accident.\\\\\\']\\']', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m df = pd.read_csv(generated_data_path)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Convert to RAGAS dataset format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ragas_dataset_from_csv = \u001b[43mEvaluationDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Evaluate using the CSV data\u001b[39;00m\n\u001b[32m     12\u001b[39m result = evaluate(\n\u001b[32m     13\u001b[39m     metrics=metrics,\n\u001b[32m     14\u001b[39m     dataset=ragas_dataset_from_csv,\n\u001b[32m     15\u001b[39m     llm=evaluator_llm,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/ragas/dataset_schema.py:251\u001b[39m, in \u001b[36mRagasDataset.from_pandas\u001b[39m\u001b[34m(cls, dataframe)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dataframe: PandasDataframe):\n\u001b[32m    250\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates an EvaluationDataset from a pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/ragas/dataset_schema.py:390\u001b[39m, in \u001b[36mEvaluationDataset.from_list\u001b[39m\u001b[34m(cls, data)\u001b[39m\n\u001b[32m    388\u001b[39m     samples.extend(MultiTurnSample(**sample) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[43msamples\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSingleTurnSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(samples=samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/ragas/dataset_schema.py:390\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    388\u001b[39m     samples.extend(MultiTurnSample(**sample) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     samples.extend(\u001b[43mSingleTurnSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(samples=samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for SingleTurnSample\nretrieved_contexts\n  Input should be a valid list [type=list_type, input_value='[\\'[\\\\\\'Ed Wood is a 199...g car accident.\\\\\\']\\']', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/list_type"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "# Load the CSV you created\n",
    "df = pd.read_csv(generated_data_path)\n",
    "\n",
    "# Convert to RAGAS dataset format\n",
    "ragas_dataset_from_csv = EvaluationDataset.from_pandas(df)\n",
    "\n",
    "# Evaluate using the CSV data\n",
    "result = evaluate(\n",
    "    metrics=metrics,\n",
    "    dataset=ragas_dataset_from_csv,\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "# Save evaluation results\n",
    "df_results = result.to_pandas()\n",
    "df.to_csv(os.path.join(project_root, 'data', 'generated', 'hotpot_qa_workflow_plan.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
