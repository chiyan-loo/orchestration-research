{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hotpot_qa\", \n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=\"/mnt/d/datasets/hotpot_qa\"\n",
    ")\n",
    "\n",
    "hf_df = pd.DataFrame(dataset)\n",
    "hf_df = hf_df.sample(n=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759722212.190676   55871 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759722212.196514   55871 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759722212.197981   55871 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759722212.199547   55871 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from agents.prompt_and_workflow_orchestration.orchestration import OrchestrationAgent\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"hotpot_qa_orchestration_{MODEL_NAME}_pilot_4\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')\n",
    "\n",
    "\n",
    "# planner_llm = ChatOllama(model=\"qwen3:8b\", temperature=0.6)\n",
    "\n",
    "planner_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.7)\n",
    "high_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.8)\n",
    "medium_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.5)\n",
    "low_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.2)\n",
    "\n",
    "agent = OrchestrationAgent(\n",
    "    planner_llm=planner_llm,\n",
    "    high_temp_llm=high_temp_llm,\n",
    "    medium_temp_llm=medium_temp_llm,\n",
    "    low_temp_llm=low_temp_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cost based on Gemini model and token usage.\n",
    "    Uses public pricing as of mid-2025.\n",
    "    \"\"\"\n",
    "    # Pricing data keyed by full model name or prefix\n",
    "    pricing = {\n",
    "        \"gemini-2.0-flash\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        # Use “pro” special logic below for gemini-2.5-pro\n",
    "    }\n",
    "\n",
    "    # Normalize model name to lower\n",
    "    m = model_name.lower()\n",
    "\n",
    "    # Special case: gemini-2.5-pro tiered pricing\n",
    "    if m.startswith(\"gemini-2.5-pro\"):\n",
    "        # threshold check on input token count\n",
    "        if input_tokens > 200_000:\n",
    "            in_rate = 2.50\n",
    "            out_rate = 15.00\n",
    "        else:\n",
    "            in_rate = 1.25\n",
    "            out_rate = 10.00\n",
    "        return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n",
    "\n",
    "    # Other known models\n",
    "    if m in pricing:\n",
    "        in_rate = pricing[m][\"input\"]\n",
    "        out_rate = pricing[m][\"output\"]\n",
    "    else:\n",
    "        # fallback / unknown handling\n",
    "        print(f\"Warning: Unknown model '{model_name}'. Using zero cost.\")\n",
    "        in_rate = 0.0\n",
    "        out_rate = 0.0\n",
    "\n",
    "    return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759722212.231674   55871 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration\n",
      "2 iteration\n",
      "3 iteration\n",
      "4 iteration\n",
      "5 iteration\n",
      "6 iteration\n",
      "7 iteration\n",
      "8 iteration\n",
      "9 iteration\n",
      "10 iteration\n",
      "11 iteration\n",
      "12 iteration\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Silence the agent's output\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(StringIO()):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m agent.generate_response_async(question, context)\n\u001b[32m     17\u001b[39m generated_dataset.append(\n\u001b[32m     18\u001b[39m     {\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     }\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i > \u001b[32m10\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:563\u001b[39m, in \u001b[36mOrchestrationAgent.generate_response_async\u001b[39m\u001b[34m(self, query, context)\u001b[39m\n\u001b[32m    553\u001b[39m initial_state = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    560\u001b[39m }\n\u001b[32m    562\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting three-phase async orchestration with custom prompts for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph.ainvoke(initial_state)\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    566\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo response generated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    567\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    568\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    569\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    570\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3112\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3109\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3110\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3114\u001b[39m     config,\n\u001b[32m   3115\u001b[39m     context=context,\n\u001b[32m   3116\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3119\u001b[39m     print_mode=print_mode,\n\u001b[32m   3120\u001b[39m     output_keys=output_keys,\n\u001b[32m   3121\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3122\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3123\u001b[39m     durability=durability,\n\u001b[32m   3124\u001b[39m     **kwargs,\n\u001b[32m   3125\u001b[39m ):\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:208\u001b[39m, in \u001b[36mOrchestrationAgent._plan_workflow_async\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    201\u001b[39m structured_llm = \u001b[38;5;28mself\u001b[39m.llm.with_structured_output(WorkflowPlan)\n\u001b[32m    203\u001b[39m planner_messages = [\n\u001b[32m    204\u001b[39m     SystemMessage(content=system_prompt),\n\u001b[32m    205\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mCreate a structured three-phase workflow plan for this query.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    206\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m structured_llm.ainvoke(planner_messages)\n\u001b[32m    210\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlanner reasoning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.reasoning\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBeginning phase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.beginning\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3291\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3289\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3290\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3291\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3292\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5724\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5717\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5718\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5719\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5722\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5723\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5725\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5726\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5727\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5728\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:417\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Any,\n\u001b[32m    415\u001b[39m ) -> BaseMessage:\n\u001b[32m    416\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    418\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    419\u001b[39m         stop=stop,\n\u001b[32m    420\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    421\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    422\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    423\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    424\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    425\u001b[39m         **kwargs,\n\u001b[32m    426\u001b[39m     )\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1036\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1027\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1033\u001b[39m     **kwargs: Any,\n\u001b[32m   1034\u001b[39m ) -> LLMResult:\n\u001b[32m   1035\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1037\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1038\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:956\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    943\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    944\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    945\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    950\u001b[39m     run_id=run_id,\n\u001b[32m    951\u001b[39m )\n\u001b[32m    953\u001b[39m input_messages = [\n\u001b[32m    954\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    955\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    957\u001b[39m     *[\n\u001b[32m    958\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    959\u001b[39m             m,\n\u001b[32m    960\u001b[39m             stop=stop,\n\u001b[32m    961\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    962\u001b[39m             **kwargs,\n\u001b[32m    963\u001b[39m         )\n\u001b[32m    964\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    965\u001b[39m     ],\n\u001b[32m    966\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    967\u001b[39m )\n\u001b[32m    968\u001b[39m exceptions = []\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1164\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1162\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1163\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1165\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1166\u001b[39m     )\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1842\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1841\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1842\u001b[39m response: GenerateContentResponse = \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(\n\u001b[32m   1843\u001b[39m     request=request,\n\u001b[32m   1844\u001b[39m     **kwargs,\n\u001b[32m   1845\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.async_client.generate_content,\n\u001b[32m   1846\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1847\u001b[39m )\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:287\u001b[39m, in \u001b[36m_achat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    280\u001b[39m params = (\n\u001b[32m    281\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    286\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _achat_with_retry(**params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:265\u001b[39m, in \u001b[36m_achat_with_retry.<locals>._achat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    267\u001b[39m         \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    268\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py:445\u001b[39m, in \u001b[36mGenerativeServiceAsyncClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m._client._validate_universe_domain()\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m rpc(\n\u001b[32m    446\u001b[39m     request,\n\u001b[32m    447\u001b[39m     retry=retry,\n\u001b[32m    448\u001b[39m     timeout=timeout,\n\u001b[32m    449\u001b[39m     metadata=metadata,\n\u001b[32m    450\u001b[39m )\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary_async.py:231\u001b[39m, in \u001b[36mAsyncRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A wrapper that calls target function with retry.\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry_target(\n\u001b[32m    232\u001b[39m     functools.partial(func, *args, **kwargs),\n\u001b[32m    233\u001b[39m     predicate=\u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    234\u001b[39m     sleep_generator=sleep_generator,\n\u001b[32m    235\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    236\u001b[39m     on_error=on_error,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary_async.py:158\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m target()\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    162\u001b[39m         \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers_async.py:86\u001b[39m, in \u001b[36m_WrappedUnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__await__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[P]:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         response = \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call.\u001b[34m__await__\u001b[39m()\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/grpc/aio/_interceptor.py:473\u001b[39m, in \u001b[36m_InterceptedUnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__await__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    472\u001b[39m     call = \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._interceptors_task.\u001b[34m__await__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     response = \u001b[38;5;28;01myield from\u001b[39;00m call.\u001b[34m__await__\u001b[39m()\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/grpc/aio/_call.py:311\u001b[39m, in \u001b[36m_UnaryResponseMixin.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Wait till the ongoing RPC request finishes.\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     response = \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_response\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    313\u001b[39m     \u001b[38;5;66;03m# Even if we caught all other CancelledError, there is still\u001b[39;00m\n\u001b[32m    314\u001b[39m     \u001b[38;5;66;03m# this corner case. If the application cancels immediately after\u001b[39;00m\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# the Call object is created, we will observe this\u001b[39;00m\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# `CancelledError`.\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cancelled():\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "generated_dataset = []\n",
    "hf_df = hf_df.reset_index(drop=True)\n",
    "\n",
    "for i, item in hf_df.iterrows():\n",
    "    print(f\"{i+1} iteration\")\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    context = item[\"context\"][\"sentences\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Silence the agent's output\n",
    "    with redirect_stdout(StringIO()):\n",
    "        response = await agent.generate_response_async(question, context)\n",
    "\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    input_tokens = response.get(\"input_tokens\", 0)\n",
    "    output_tokens = response.get(\"output_tokens\", 0)\n",
    "    cost = calculate_cost(MODEL_NAME, input_tokens, output_tokens)\n",
    "\n",
    "    generated_dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"contexts\": [str(item) for item in context],\n",
    "            \"response\": response[\"content\"].strip(),\n",
    "            \"ground_truth\": answer,\n",
    "            \"workflow_plan\": response[\"workflow_plan\"],\n",
    "            \"planner_reasoning\": response[\"planner_reasoning\"],\n",
    "            \"custom_prompts\": response[\"custom_prompts\"],\n",
    "            \"latency\": latency,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost\": cost,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(generated_dataset)\n",
    "df.to_parquet(generated_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(generated_dataset)\n",
    "df.to_parquet(generated_data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
