{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hotpot_qa\", \n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=\"/mnt/d/datasets/hotpot_qa\"\n",
    ")\n",
    "\n",
    "hf_df = pd.DataFrame(dataset)\n",
    "# hf_df = hf_df.sample(n=300, random_state=42)\n",
    "hf_df = hf_df.sample(n=10, random_state=67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759818256.928960   56577 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759818256.930843   56577 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759818256.932940   56577 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759818256.934608   56577 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from agents.prompt_and_workflow_orchestration.orchestration import OrchestrationAgent\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"hotpot_qa_orchestration_{MODEL_NAME}_pilot_5\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')\n",
    "\n",
    "\n",
    "# planner_llm = ChatOllama(model=\"qwen3:8b\", temperature=0.6)\n",
    "\n",
    "planner_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.7)\n",
    "high_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.8)\n",
    "medium_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.5)\n",
    "low_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.2)\n",
    "\n",
    "agent = OrchestrationAgent(\n",
    "    planner_llm=planner_llm,\n",
    "    high_temp_llm=high_temp_llm,\n",
    "    medium_temp_llm=medium_temp_llm,\n",
    "    low_temp_llm=low_temp_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cost based on Gemini model and token usage.\n",
    "    Uses public pricing as of mid-2025.\n",
    "    \"\"\"\n",
    "    # Pricing data keyed by full model name or prefix\n",
    "    pricing = {\n",
    "        \"gemini-2.0-flash\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        # Use “pro” special logic below for gemini-2.5-pro\n",
    "    }\n",
    "\n",
    "    # Normalize model name to lower\n",
    "    m = model_name.lower()\n",
    "\n",
    "    # Special case: gemini-2.5-pro tiered pricing\n",
    "    if m.startswith(\"gemini-2.5-pro\"):\n",
    "        # threshold check on input token count\n",
    "        if input_tokens > 200_000:\n",
    "            in_rate = 2.50\n",
    "            out_rate = 15.00\n",
    "        else:\n",
    "            in_rate = 1.25\n",
    "            out_rate = 10.00\n",
    "        return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n",
    "\n",
    "    # Other known models\n",
    "    if m in pricing:\n",
    "        in_rate = pricing[m][\"input\"]\n",
    "        out_rate = pricing[m][\"output\"]\n",
    "    else:\n",
    "        # fallback / unknown handling\n",
    "        print(f\"Warning: Unknown model '{model_name}'. Using zero cost.\")\n",
    "        in_rate = 0.0\n",
    "        out_rate = 0.0\n",
    "\n",
    "    return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759818256.973404   56577 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration\n",
      "2 iteration\n",
      "3 iteration\n",
      "4 iteration\n",
      "5 iteration\n",
      "6 iteration\n",
      "7 iteration\n",
      "8 iteration\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Silence the agent's output\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(StringIO()):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m agent.generate_response_async(query=question, context=context, callback=callback)\n\u001b[32m     22\u001b[39m end_time = time.time()\n\u001b[32m     23\u001b[39m latency = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:584\u001b[39m, in \u001b[36mOrchestrationAgent.generate_response_async\u001b[39m\u001b[34m(self, query, context, callback)\u001b[39m\n\u001b[32m    573\u001b[39m initial_state = {\n\u001b[32m    574\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m    575\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    580\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m\"\u001b[39m: callback,\n\u001b[32m    581\u001b[39m }\n\u001b[32m    583\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting three-phase async orchestration with custom prompts for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph.ainvoke(initial_state)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    587\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo response generated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    588\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    589\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    590\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    591\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3112\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3109\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3110\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3114\u001b[39m     config,\n\u001b[32m   3115\u001b[39m     context=context,\n\u001b[32m   3116\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3119\u001b[39m     print_mode=print_mode,\n\u001b[32m   3120\u001b[39m     output_keys=output_keys,\n\u001b[32m   3121\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3122\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3123\u001b[39m     durability=durability,\n\u001b[32m   3124\u001b[39m     **kwargs,\n\u001b[32m   3125\u001b[39m ):\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:548\u001b[39m, in \u001b[36mOrchestrationAgent._execute_end_async\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# Wrap synchronous call in executor\u001b[39;00m\n\u001b[32m    547\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m improved_response = \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.refiner.generate_response(\n\u001b[32m    551\u001b[39m         query=query,\n\u001b[32m    552\u001b[39m         current_response=current_response,\n\u001b[32m    553\u001b[39m         context=context,\n\u001b[32m    554\u001b[39m         critic_system_prompt=critic_prompt,\n\u001b[32m    555\u001b[39m         editor_system_prompt=editor_prompt,\n\u001b[32m    556\u001b[39m         callback=callback\n\u001b[32m    557\u001b[39m     )\n\u001b[32m    558\u001b[39m )\n\u001b[32m    560\u001b[39m messages.append(AIMessage(content=improved_response[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    562\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Refiner result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimproved_response[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m250\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "generated_dataset = []\n",
    "hf_df = hf_df.reset_index(drop=True)\n",
    "\n",
    "for i, item in hf_df.iterrows():\n",
    "    print(f\"{i+1} iteration\")\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    context = item[\"context\"][\"sentences\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "    # Silence the agent's output\n",
    "    with redirect_stdout(StringIO()):\n",
    "        response = await agent.generate_response_async(query=question, context=context, callback=callback)\n",
    "\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    input_tokens = callback.usage_metadata[MODEL_NAME][\"input_tokens\"]\n",
    "    output_tokens = callback.usage_metadata[MODEL_NAME][\"output_tokens\"]\n",
    "    total_tokens = callback.usage_metadata[MODEL_NAME][\"total_tokens\"]\n",
    "    cost = calculate_cost(MODEL_NAME, input_tokens, output_tokens)\n",
    "\n",
    "    generated_dataset.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"contexts\": [str(item) for item in context],\n",
    "            \"response\": response[\"content\"].strip(),\n",
    "            \"ground_truth\": answer,\n",
    "            \"workflow_plan\": response[\"workflow_plan\"],\n",
    "            \"planner_reasoning\": response[\"planner_reasoning\"],\n",
    "            \"custom_prompts\": response[\"custom_prompts\"],\n",
    "            \"latency\": latency,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"cost\": cost,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(generated_dataset)\n",
    "df.to_parquet(generated_data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
