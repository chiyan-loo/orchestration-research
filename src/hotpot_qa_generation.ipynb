{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mounty-ed/stuff/orchestration_research/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hotpot_qa\", \n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=\"/mnt/d/datasets/hotpot_qa\"\n",
    ")\n",
    "\n",
    "hf_df = pd.DataFrame(dataset)\n",
    "hf_df = hf_df.sample(n=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760060237.295620    2122 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760060237.325486    2122 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760060237.327031    2122 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760060237.328694    2122 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from agents.prompt_and_workflow_orchestration.orchestration import OrchestrationAgent\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"hotpot_qa_orchestration_{MODEL_NAME}\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "generated_data_path = os.path.join(project_root, 'data', 'generated', f'{EXPERIMENT_NAME}.parquet')\n",
    "\n",
    "\n",
    "# planner_llm = ChatOllama(model=\"qwen3:8b\", temperature=0.6)\n",
    "\n",
    "planner_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.7)\n",
    "high_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.8)\n",
    "medium_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.5)\n",
    "low_temp_llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.2)\n",
    "\n",
    "agent = OrchestrationAgent(\n",
    "    planner_llm=planner_llm,\n",
    "    high_temp_llm=high_temp_llm,\n",
    "    medium_temp_llm=medium_temp_llm,\n",
    "    low_temp_llm=low_temp_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cost based on Gemini model and token usage.\n",
    "    Uses public pricing as of mid-2025.\n",
    "    \"\"\"\n",
    "    # Pricing data keyed by full model name or prefix\n",
    "    pricing = {\n",
    "        \"gemini-2.0-flash\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        # Use “pro” special logic below for gemini-2.5-pro\n",
    "    }\n",
    "\n",
    "    # Normalize model name to lower\n",
    "    m = model_name.lower()\n",
    "\n",
    "    # Special case: gemini-2.5-pro tiered pricing\n",
    "    if m.startswith(\"gemini-2.5-pro\"):\n",
    "        # threshold check on input token count\n",
    "        if input_tokens > 200_000:\n",
    "            in_rate = 2.50\n",
    "            out_rate = 15.00\n",
    "        else:\n",
    "            in_rate = 1.25\n",
    "            out_rate = 10.00\n",
    "        return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n",
    "\n",
    "    # Other known models\n",
    "    if m in pricing:\n",
    "        in_rate = pricing[m][\"input\"]\n",
    "        out_rate = pricing[m][\"output\"]\n",
    "    else:\n",
    "        # fallback / unknown handling\n",
    "        print(f\"Warning: Unknown model '{model_name}'. Using zero cost.\")\n",
    "        in_rate = 0.0\n",
    "        out_rate = 0.0\n",
    "\n",
    "    return (input_tokens / 1_000_000) * in_rate + (output_tokens / 1_000_000) * out_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760060237.406832    2122 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Processing:   6%|▌         | 18/300 [07:03<1:50:34, 23.53s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Silence the agent's output\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(StringIO()):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m agent.generate_response_async(query=question, context=context, callback=callback)\n\u001b[32m     22\u001b[39m end_time = time.time()\n\u001b[32m     23\u001b[39m latency = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:584\u001b[39m, in \u001b[36mOrchestrationAgent.generate_response_async\u001b[39m\u001b[34m(self, query, context, callback)\u001b[39m\n\u001b[32m    573\u001b[39m initial_state = {\n\u001b[32m    574\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m    575\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    580\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m\"\u001b[39m: callback,\n\u001b[32m    581\u001b[39m }\n\u001b[32m    583\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting three-phase async orchestration with custom prompts for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph.ainvoke(initial_state)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    587\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo response generated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    588\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mworkflow_plan\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    589\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mplanner_reasoning\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    590\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m: response[\u001b[33m\"\u001b[39m\u001b[33mcustom_prompts\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    591\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3112\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3109\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3110\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3114\u001b[39m     config,\n\u001b[32m   3115\u001b[39m     context=context,\n\u001b[32m   3116\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3119\u001b[39m     print_mode=print_mode,\n\u001b[32m   3120\u001b[39m     output_keys=output_keys,\n\u001b[32m   3121\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3122\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3123\u001b[39m     durability=durability,\n\u001b[32m   3124\u001b[39m     **kwargs,\n\u001b[32m   3125\u001b[39m ):\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:480\u001b[39m, in \u001b[36mOrchestrationAgent._execute_middle_async\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    477\u001b[39m         tasks.append(run_debater())\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Execute all middle agents in parallel\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# Add results to messages in the order they completed\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_name, response \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:435\u001b[39m, in \u001b[36mOrchestrationAgent._execute_middle_async.<locals>.run_predictor\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Wrap synchronous call in executor\u001b[39;00m\n\u001b[32m    434\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.predictor.generate_response(\n\u001b[32m    438\u001b[39m         query=query,\n\u001b[32m    439\u001b[39m         context=context,\n\u001b[32m    440\u001b[39m         system_prompt=custom_prompt,\n\u001b[32m    441\u001b[39m         callback=callback\n\u001b[32m    442\u001b[39m     )\n\u001b[32m    443\u001b[39m )\n\u001b[32m    445\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Predictor result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m250\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mpredictor\u001b[39m\u001b[33m\"\u001b[39m, response[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/orchestration.py:437\u001b[39m, in \u001b[36mOrchestrationAgent._execute_middle_async.<locals>.run_predictor.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Wrap synchronous call in executor\u001b[39;00m\n\u001b[32m    434\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m    435\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m )\n\u001b[32m    445\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Predictor result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m250\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mpredictor\u001b[39m\u001b[33m\"\u001b[39m, response[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stuff/orchestration_research/src/agents/prompt_and_workflow_orchestration/predictor.py:49\u001b[39m, in \u001b[36mPredictor.generate_response\u001b[39m\u001b[34m(self, query, context, system_prompt, callback)\u001b[39m\n\u001b[32m     44\u001b[39m response = \u001b[38;5;28mself\u001b[39m.llm.invoke(messages, config={\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: [callback]})\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull reasoning response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m,\n\u001b[32m     50\u001b[39m }\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'answer'",
      "During task with name 'execute_middle' and id '12b02130-365c-2e45-8cb7-686d06ba15ef'"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "generated_dataset = []\n",
    "hf_df = hf_df.reset_index(drop=True)\n",
    "\n",
    "for i, item in tqdm(hf_df.iterrows(), total=len(hf_df), desc=\"Processing\"):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answer\"]\n",
    "    context = item[\"context\"][\"sentences\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "    try:\n",
    "        # Silence the agent's output\n",
    "        with redirect_stdout(StringIO()):\n",
    "            response = await agent.generate_response_async(query=question, context=context, callback=callback)\n",
    "\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "\n",
    "        input_tokens = callback.usage_metadata[MODEL_NAME][\"input_tokens\"]\n",
    "        output_tokens = callback.usage_metadata[MODEL_NAME][\"output_tokens\"]\n",
    "        total_tokens = callback.usage_metadata[MODEL_NAME][\"total_tokens\"]\n",
    "        cost = calculate_cost(MODEL_NAME, input_tokens, output_tokens)\n",
    "\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"user_input\": question,\n",
    "                \"contexts\": [str(item) for item in context],\n",
    "                \"response\": response[\"content\"].strip(),\n",
    "                \"ground_truth\": answer,\n",
    "                \"workflow_plan\": response[\"workflow_plan\"],\n",
    "                \"planner_reasoning\": response[\"planner_reasoning\"],\n",
    "                \"custom_prompts\": response[\"custom_prompts\"],\n",
    "                \"workflow_plan_failed\": response.get(\"workflow_plan_failed\", False),\n",
    "                \"prompt_optimization_failed\": response.get(\"prompt_optimization_failed\", False),\n",
    "                \"latency\": latency,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"cost\": cost,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors and continue\n",
    "        print(f\"✗ Unexpected error for item {i}: {e}\")\n",
    "\n",
    "        # Add entry with error information\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"user_input\": question,\n",
    "                \"contexts\": [str(item) for item in context],\n",
    "                \"response\": f\"ERROR: {str(e)}\",\n",
    "                \"ground_truth\": answer,\n",
    "                \"workflow_plan\": None,\n",
    "                \"planner_reasoning\": None,\n",
    "                \"custom_prompts\": None,\n",
    "                \"workflow_plan_failed\": True,\n",
    "                \"prompt_optimization_failed\": True,\n",
    "                \"latency\": 0,\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "                \"cost\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(generated_dataset)\n",
    "df.to_parquet(generated_data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
