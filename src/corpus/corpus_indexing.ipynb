{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ffaa0e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063013fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636c11f",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"florin-hf/wiki_dump2018_nq_open\"\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "MAX_SAMPLES = 1000  # Set to a number like 10000 for testing, None for full dataset\n",
    "TEXT_COLUMN = \"text\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create persist directory\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Vector store will be saved to: {os.path.abspath(PERSIST_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_SAMPLES:\n",
    "    dataset = load_dataset(DATASET_NAME, split=f\"train[:{MAX_SAMPLES}]\")\n",
    "    print(f\"Loaded {len(dataset)} samples (limited for testing)\")\n",
    "else:\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    print(f\"Loaded full dataset: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4344d",
   "metadata": {},
   "source": [
    "## Indexing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset into Document objects\n",
    "documents = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    # Create metadata from other columns\n",
    "    metadata = {k: v for k, v in item.items() \n",
    "                if k != TEXT_COLUMN and isinstance(v, (str, int, float, bool))}\n",
    "    metadata['source'] = f\"{DATASET_NAME}_{i}\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=item[TEXT_COLUMN],\n",
    "        metadata=metadata\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"Processed {i + 1} documents...\")\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma vector store\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Add documents to vector store in batches\n",
    "print(f\"Adding chunks to vector store...\")\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    vectorstore.add_documents(batch)\n",
    "    \n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"Added {min(i + batch_size, len(chunks))}/{len(chunks)} chunks...\")\n",
    "\n",
    "# Persist the vector store\n",
    "vectorstore.persist()\n",
    "print(f\"Vector store persisted to: {PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8762b",
   "metadata": {},
   "source": [
    "## Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"artificial intelligence\"\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"Found {len(results)} results:\")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
