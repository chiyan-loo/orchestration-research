{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf89a09",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3080665",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = \"florin-hf/wiki_dump2018_nq_open\"\n",
    "PERSIST_DIR = \"/mnt/d/datasets/wiki_dump2018_nq_open/chroma_db\"\n",
    "CACHE_DIR = \"/mnt/d/datasets/wiki_dump2018_nq_open\"\n",
    "MAX_SAMPLES = None  # Set to None for full dataset\n",
    "TEXT_COLUMN = \"text\"\n",
    "BATCH_SIZE = 1000  # Process documents in batches\n",
    "CHUNK_BATCH_SIZE = 500  # Add chunks to vector store in smaller batches\n",
    "\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Vector store will be saved to: {os.path.abspath(PERSIST_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f996d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with streaming for memory efficiency\n",
    "if MAX_SAMPLES:\n",
    "    dataset = load_dataset(DATASET_NAME, split=f\"train[:{MAX_SAMPLES}]\", streaming=False, cache_dir=CACHE_DIR)\n",
    "    total_samples = MAX_SAMPLES\n",
    "else:\n",
    "    # For large datasets, use streaming\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True, cache_dir=CACHE_DIR)\n",
    "    total_samples = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ba11e",
   "metadata": {},
   "source": [
    "## Indexing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b68b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset in memory-efficient batches\n",
    "def process_batch(batch_items, start_idx):\n",
    "    \"\"\"Process a batch of items and return chunks\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for i, item in enumerate(batch_items):\n",
    "        metadata = {k: v for k, v in item.items() \n",
    "                   if k != TEXT_COLUMN and isinstance(v, (str, int, float, bool))}\n",
    "        metadata['source'] = f\"{DATASET_NAME}_{start_idx + i}\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=item[TEXT_COLUMN],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Clear documents from memory\n",
    "    del documents\n",
    "    gc.collect()\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d562902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop with batch processing\n",
    "processed_docs = 0\n",
    "total_chunks = 0\n",
    "batch_items = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    batch_items.append(item)\n",
    "    \n",
    "    # Process when batch is full or at end of dataset\n",
    "    if len(batch_items) >= BATCH_SIZE or (MAX_SAMPLES and i == MAX_SAMPLES - 1):\n",
    "        # Process current batch\n",
    "        chunks = process_batch(batch_items, processed_docs)\n",
    "        \n",
    "        # Add chunks to vector store in smaller batches\n",
    "        for j in range(0, len(chunks), CHUNK_BATCH_SIZE):\n",
    "            chunk_batch = chunks[j:j + CHUNK_BATCH_SIZE]\n",
    "            vectorstore.add_documents(chunk_batch)\n",
    "        \n",
    "        processed_docs += len(batch_items)\n",
    "        total_chunks += len(chunks)\n",
    "        \n",
    "        print(f\"Processed {processed_docs} documents, created {total_chunks} chunks\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunks\n",
    "        batch_items = []\n",
    "        gc.collect()\n",
    "        \n",
    "        # Break if we've reached max samples\n",
    "        if MAX_SAMPLES and processed_docs >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "print(f\"\\nâœ… Completed processing {processed_docs} documents into {total_chunks} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the vector store\n",
    "vectorstore.persist()\n",
    "print(f\"Vector store persisted to: {PERSIST_DIR}\")\n",
    "print(f\"Total documents processed: {processed_docs}\")\n",
    "print(f\"Total chunks created: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ad69c",
   "metadata": {},
   "source": [
    "## Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c569aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"George Washington\"\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"Found {len(results)} results:\")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
