{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf89a09",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b3c07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3080665",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec2b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store will be saved to: /mnt/d/datasets\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = \"florin-hf/wiki_dump2018_nq_open\"\n",
    "PERSIST_DIR = \"/mnt/d/datasets\"\n",
    "CACHE_DIR = \"/mnt/d/datasets/wiki_dump2018_nq_open\"\n",
    "MAX_SAMPLES = None  # Set to None for full dataset\n",
    "TEXT_COLUMN = \"text\"\n",
    "BATCH_SIZE = 100000  # Process documents in batches\n",
    "CHUNK_BATCH_SIZE = 500  # Add chunks to vector store in smaller batches\n",
    "COLLECTION_NAME = \"wiki_dump2018_nq_open\"\n",
    "\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Vector store will be saved to: {os.path.abspath(PERSIST_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f996d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with streaming for memory efficiency\n",
    "if MAX_SAMPLES:\n",
    "    dataset = load_dataset(DATASET_NAME, split=f\"train[:{MAX_SAMPLES}]\", streaming=False, cache_dir=CACHE_DIR)\n",
    "    total_samples = MAX_SAMPLES\n",
    "else:\n",
    "    # For large datasets, use streaming\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True, cache_dir=CACHE_DIR)\n",
    "    total_samples = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ba11e",
   "metadata": {},
   "source": [
    "## Indexing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0698ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection 'wiki_dump2018_nq_open'\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "# Initialize local Qdrant client\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists\")\n",
    "except Exception:\n",
    "    # Collection doesn't exist, create it\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=384,  # all-MiniLM-L6-v2 embedding dimension\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    print(f\"Created collection '{COLLECTION_NAME}'\")\n",
    "\n",
    "# Initialize Qdrant vectorstore\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b68b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset in memory-efficient batches\n",
    "def process_batch(batch_items, start_idx):\n",
    "    \"\"\"Process a batch of items and return chunks\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for i, item in enumerate(batch_items):\n",
    "        metadata = {k: v for k, v in item.items() \n",
    "                   if k != TEXT_COLUMN and isinstance(v, (str, int, float, bool))}\n",
    "        metadata['source'] = f\"{DATASET_NAME}_{start_idx + i}\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=item[TEXT_COLUMN],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Clear documents from memory\n",
    "    del documents\n",
    "    gc.collect()\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d562902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents at iteration: 99999\n",
      "Adding documents\n",
      "Processed 100000 documents, created 100002 chunks\n",
      "Processing documents at iteration: 199999\n",
      "Adding documents\n",
      "Processed 200000 documents, created 200002 chunks\n",
      "Processing documents at iteration: 299999\n",
      "Adding documents\n",
      "Processed 300000 documents, created 300003 chunks\n",
      "Processing documents at iteration: 399999\n",
      "Adding documents\n",
      "Processed 400000 documents, created 400009 chunks\n",
      "Processing documents at iteration: 499999\n",
      "Adding documents\n",
      "Processed 500000 documents, created 500010 chunks\n",
      "Processing documents at iteration: 599999\n",
      "Adding documents\n",
      "Processed 600000 documents, created 600010 chunks\n",
      "Processing documents at iteration: 699999\n",
      "Adding documents\n",
      "Processed 700000 documents, created 700012 chunks\n",
      "Processing documents at iteration: 799999\n",
      "Adding documents\n",
      "Processed 800000 documents, created 800017 chunks\n",
      "Processing documents at iteration: 899999\n",
      "Adding documents\n",
      "Processed 900000 documents, created 900022 chunks\n",
      "Processing documents at iteration: 999999\n",
      "Adding documents\n",
      "Processed 1000000 documents, created 1000024 chunks\n",
      "Processing documents at iteration: 1099999\n",
      "Adding documents\n",
      "Processed 1100000 documents, created 1100027 chunks\n",
      "Processing documents at iteration: 1199999\n",
      "Adding documents\n",
      "Processed 1200000 documents, created 1200027 chunks\n",
      "Processing documents at iteration: 1299999\n",
      "Adding documents\n",
      "Processed 1300000 documents, created 1300028 chunks\n",
      "Processing documents at iteration: 1399999\n",
      "Adding documents\n"
     ]
    }
   ],
   "source": [
    "# Main processing loop with batch processing\n",
    "processed_docs = 0\n",
    "total_chunks = 0\n",
    "batch_items = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    batch_items.append(item)\n",
    "    \n",
    "    # Process when batch is full or at end of dataset\n",
    "    if len(batch_items) >= BATCH_SIZE or (MAX_SAMPLES and i == MAX_SAMPLES - 1):\n",
    "        print(f\"Processing documents at iteration: {i+1}\")\n",
    "        # Process current batch\n",
    "        chunks = process_batch(batch_items, processed_docs)\n",
    "        \n",
    "        print(f\"Adding documents\")\n",
    "        # Add chunks to vector store in smaller batches\n",
    "        for j in range(0, len(chunks), CHUNK_BATCH_SIZE):\n",
    "            chunk_batch = chunks[j:j + CHUNK_BATCH_SIZE]\n",
    "            vectorstore.add_documents(chunk_batch)\n",
    "        \n",
    "        processed_docs += len(batch_items)\n",
    "        total_chunks += len(chunks)\n",
    "        \n",
    "        print(f\"Processed {processed_docs} documents, created {total_chunks} chunks\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunks\n",
    "        batch_items = []\n",
    "        gc.collect()\n",
    "        \n",
    "        # Break if we've reached max samples\n",
    "        if MAX_SAMPLES and processed_docs >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "print(f\"\\nâœ… Completed processing {processed_docs} documents into {total_chunks} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vector store persisted to: {PERSIST_DIR}\")\n",
    "print(f\"Total documents processed: {processed_docs}\")\n",
    "print(f\"Total chunks created: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ad69c",
   "metadata": {},
   "source": [
    "## Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c569aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"George Washington\"\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"Found {len(results)} results:\")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
