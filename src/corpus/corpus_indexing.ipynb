{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ffaa0e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063013fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636c11f",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ce8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store will be saved to: /mnt/d/datasets/wiki_dump2018_nq_open/chroma_db\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"florin-hf/wiki_dump2018_nq_open\"\n",
    "CACHE_DIR = \"/mnt/d/datasets/wiki_dump2018_nq_open\"\n",
    "PERSIST_DIR = \"/mnt/d/datasets/wiki_dump2018_nq_open/chroma_db\"\n",
    "MAX_SAMPLES = None  # Set to a number like 10000 for testing, None for full dataset\n",
    "TEXT_COLUMN = \"text\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Create persist directory\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Vector store will be saved to: {os.path.abspath(PERSIST_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d87ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae255882571458c8086d68e60cbb8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc81450bc6f04136b629127fbfe0c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a15c37410844c6cbf656cd3369a9d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    DATASET_NAME, \n",
    "    split=f\"train\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_SAMPLES:\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4344d",
   "metadata": {},
   "source": [
    "## Indexing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4d73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 documents\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset into Document objects\n",
    "documents = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    # Create metadata from other columns\n",
    "    metadata = {k: v for k, v in item.items() \n",
    "                if k != TEXT_COLUMN and isinstance(v, (str, int, float, bool))}\n",
    "    metadata['source'] = f\"{DATASET_NAME}_{i}\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=item[TEXT_COLUMN],\n",
    "        metadata=metadata\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"Processed {i + 1} documents...\")\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd5160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4058f02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding chunks to vector store...\n",
      "Added 1000/1000 chunks...\n",
      "Vector store persisted to: /mnt/d/datasets/wiki_dump2018_nq_open/chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3926/761089995.py:19: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma vector store\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Add documents to vector store in batches\n",
    "print(f\"Adding chunks to vector store...\")\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    vectorstore.add_documents(batch)\n",
    "    \n",
    "    if (i + batch_size) % 1000 == 0:\n",
    "        print(f\"Added {min(i + batch_size, len(chunks))}/{len(chunks)} chunks...\")\n",
    "\n",
    "# Persist the vector store\n",
    "vectorstore.persist()\n",
    "print(f\"Vector store persisted to: {PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8762b",
   "metadata": {},
   "source": [
    "## Test Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a1d954",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mWho invented the light bulb?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mvectorstore\u001b[49m.similarity_search(test_query, k=\u001b[32m3\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "test_query = \"Who invented the light bulb?\"\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"Found {len(results)} results:\")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
